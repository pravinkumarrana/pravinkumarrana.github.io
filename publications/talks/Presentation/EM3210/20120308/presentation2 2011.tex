\documentclass[8pt]{beamer}
\mode<presentation>
{
  \usetheme{boxes}
  \setbeamercovered{transparent}
}
\usepackage{subfig}
\usepackage{amsmath,epsfig, psfrag, hyperref}
\usepackage{amssymb, algorithm, algorithmicx}
\usepackage{algpseudocode}
\usepackage{caption}
\usepackage[english]{babel}
\usepackage[latin1]{inputenc}
\usepackage{times}
\usepackage[T1]{fontenc}
\title[Project Assignment \#2] {\textbf{Project Assignment \#2}}
\author{\textbf{FEM1230 Estimation Theory}}



\begin{document}

\begin{frame}
  \titlepage
  \begin{center}
  \textcolor[rgb]{0.29,0.29,0.29}{\textbf{Pravin Kumar Rana}} \textcolor[rgb]{0.29,0.29,0.29}{\\SIP, School of Electrical Engineering\\ Kungliga Tekniska Högskolan \\ SE-$100 44$ Stockholm}
  \end{center}
\end{frame}

\begin{frame}{Outline}
\begin{block}{}
\begin{itemize}
  \item Data Model\vspace{3mm}
  \item Maximum Likelihood Estimation\vspace{3mm}
  \item Mean Square Error Bounds With Affine Bias\vspace{3mm}
  \item Mean Square Error Of SNR Affine Biased Estimator
\end{itemize}

\end{block}

\end{frame}

%Measurements and Data Model
\section{Data Model}
\begin{frame}{Data Model}
\begin{itemize}
  \item Assume a data model
    \begin{equation}
    x[n] = A + w[n] \quad n = 1, 2, \ldots, N-1
    \end{equation}
    where A is the unknown DC level and $w[n] \sim \mathcal{N}(0,\sigma^2)$ is additive noise with unknown variance $\sigma^2$,
    \begin{equation}
    \theta = \left[ A ~\sigma^2\right]
    \end{equation}
   \item The SNR $\alpha =\frac{A^2}{\sigma^2}$ is lower bounded for any unbiased estimator $\hat{\alpha}$ by CRLB,
   \begin{equation}
        var(\alpha)\geq \frac{4\alpha+2\alpha^2}{N}.
    \end{equation}
    \item[Problem:] Signal--to--noise ratio (SNR) estimation of the data model with unknown A and $\sigma^2$.
\end{itemize}
\end{frame}

% Maximum Likelihood Estimation
\section{Maximum Likelihood Estimation}
\begin{frame}{Maximum Likelihood Estimation (MLE)}
\begin{itemize}
\item Using the probability density function of the given data model
\begin{align}
\displaystyle{p(\mathbf{x}; \mathbf{\theta})} &= \displaystyle{\frac{1}{(2\pi\sigma^2)^{\frac{N}{2}}}\exp\left[\frac{-1}{2\sigma^2}\sum_{n=0}^{N-1}({x[n]}-A)^{2}\right]},
\end{align}
where $\mathbf{x} = [x[0]\ldots x[N-1]]^{T}$ and by using the invariance property of the MLE, the MLE of SNR $\alpha$ is given by
\begin{align}\label{eq:12}
\displaystyle{ \hat{\alpha}_{ML} } &= \displaystyle{\widehat{\left(\frac{A^2}{\sigma^2}\right)}_{ML}} = \displaystyle{\frac{\hat{A}^2_{ML}}{\hat{\sigma}^2_{ML}}}=\frac{\displaystyle{\left(\frac{1}{N}\sum_{n=0}^{N-1}x[n]\right)^2}}{\displaystyle{\frac{1}{N}\sum_{n=0}^{N-1}\left({x[n]}-\frac{1}{N}\sum_{n=0}^{N-1}x[n]\right)^{2}}}.
\end{align}

\end{itemize}
 \end{frame}

\begin{frame}{Maximum Likelihood Estimation (MLE)}
 \begin{itemize}
    \item The PDF of $\hat{A}^2_{ML}$ is \textcolor[rgb]{0.00,0.00,1.00}{non-central chi-squared} $\mathcal{X'}^{2}_{\kappa_{1}}$ with degree of freedom $\kappa_{1} = 1$ and non-centrality parameter $\lambda = N\alpha$ because $\displaystyle{\hat{A}_{ML}} \sim \displaystyle{\mathcal{N}(A,{\sigma^2}/{N})}$.\vspace{0.3cm}
    \item The PDF of $\hat{\sigma}^2$ is \textcolor[rgb]{0.00,0.00,1.00}{central chi-squared}  $\mathcal{X}^2_{\kappa_{2}}$ with degree of freedom $\kappa_{2} = (N-1)$.\vspace{0.3cm}
    \item Therefore the PDF of $\hat{\alpha}_{ML}$ is a \textcolor[rgb]{0.00,0.00,1.00}{non-central chi-squared},
\[
  \displaystyle{f(\hat{\alpha}_{ML})}  = \left\{
  \begin{array}{l l}
    \displaystyle{\frac{e^{-\frac{N\alpha}{2}}}{N-1}  \sum^{\infty}_{\dot{\imath} = 0} \frac{(N\alpha/2)^{\dot{\imath}}\Gamma \left(\dot{\imath}+\frac{N}{2} \right)}{ \dot{\imath}! \Gamma \left( \frac{1+2\dot{\imath}}{2} \right)\Gamma \left( \frac{N-1}{2} \right)}\frac{\hat{\alpha}^{(\dot{\imath}-\frac{1}{2})}}{[ 1+ \hat{\alpha}]^{(\dot{\imath}+\frac{N}{2})}},} & \text{$\alpha \geq$0}\\
     0 & \text{$\alpha <$0}
  \end{array} \right.
\] with the expected value

\begin{align}
  \displaystyle{\mathrm{E}\{\hat{\alpha}_{ML}\}} = \displaystyle{\frac{N}{N-3}\left(\alpha+\frac{1}{N}\right)} & \quad \text{N$>$3}
\end{align}
 and variance
\begin{align}
  \displaystyle{var\{\hat{\alpha}_{ML}\}}  =\displaystyle{\frac{\left[2\alpha^2+4\alpha\left(1-\frac{2}{N}\right)+\frac{2}{N}\left(1-\frac{2}{N}\right)\right]}{N(1-\frac{3}{N})^2(1-\frac{5}{N})}} & \quad \text{N$>$5}
\end{align}
 \end{itemize}
\end{frame}


\begin{frame}{Maximum Likelihood Estimation (MLE)}
\small{\begin{itemize}
  \item  For $N\rightarrow \infty$, The ML estimator $\hat{\alpha}_{ML}$ is asymptotically efficient,
\begin{align}\label{eq:21}
\displaystyle{var\{\hat{\alpha}_{ML}\}} &\displaystyle{ \overset{N \rightarrow \infty}{\longrightarrow} CRLB} \end{align} and the PDF of $\hat{\alpha}_{ML}$ is obey a Gaussian distribution for $N\rightarrow \infty$ from the central limit theorem. \vspace{0.2cm}
 \item The following figure compare the ML MSE to the CRLB as a function of $\alpha$ for finite N.
%\vspace{-2mm}
\begin{figure}[!b]
    \psfrag{a}[c][c][0.7][0]{~~~~~~~CRLB}
    \psfrag{b}[c][c][0.7][0]{~~~~~~~MLE}
    \psfrag{c}[c][c][0.7][0]{SNR ($\alpha$) (dB)}
    \psfrag{d}[c][c][0.7][0]{MSE ($\alpha$) (dB)}
    \centerline{\psfig{figure=fig1,width=5cm, height=5cm}}
    \caption{The CRLB and the ML MSE as the function of $\alpha$ for N~$=200$.}
	\label{fig:plot1}
\end{figure}
\end{itemize}}
\end{frame}

%
%mle
\section{Mean Square Error Bounds With Affine Bias}
\begin{frame}{Mean Square Error Bounds (MSEB) With Affine Bias}

\begin{itemize}
\item CRLB on the MSE of an unbiased estimator $\hat{\theta}$ of ${\theta}$ is
\begin{align}\label{eq:23}
MSEB(0,0,\theta)=\displaystyle{E\{ \| \hat{\theta}-\theta \|^{2} \}} \geq\displaystyle{Tr(J^{-1}(\theta))},
\end{align}
where $J(\theta)$ is the Fisher information matrix.\vspace{0.2cm}

\item \textcolor[rgb]{0.00,0.00,1.00}{Affine MSE bound} on the MSE of an estimator with bias $E\{\hat{\theta}\}-\theta=M\theta+u$ is
\begin{align}\nonumber
\displaystyle{MSEB(M,u,\theta)} &=\displaystyle{\|E\{\hat{\theta}\}-\theta\|^2+Tr((I+D(\theta))J^{-1}(\theta)(I+D(\theta))^{H})}\\
                                &= \displaystyle{(M\theta+u)^{H}(M\theta+u)} + \displaystyle{Tr(I+M)J^{-1}(\theta)(I+M)^{H}},
\end{align} for some matrix M and $u\in \mathcal{U}$, where $D(\theta)= \frac{\partial(E\{\hat{\theta}\}-\theta)}{ \partial\theta}$.\vspace{0.2cm}
\end{itemize}
\end{frame}

\begin{frame}{Mean Square Error Bounds (MSEB) With Affine Bias}
\begin{block}
{\textcolor[rgb]{1.0,0.00,0.00}{Theorem 1 [Y. C. Eldar (2008)]}}
\end{block}
\begin{itemize}
%\item If M, u satisfy
%\begin{align}\label{eq:27}
%\begin{array}{ccc}
%  MSEB(M,u,\theta)   & \leq MSEB(0,0,\theta) & \quad \forall \text{$\theta\in\mathcal{U}$}\\
%  MSEB(M',u',\theta) & \leq MSEB(M,u,\theta) & \quad \forall \text{$\theta\in\mathcal{U}$}\\
%  ~~~ & \Rightarrow (M',u') =(M,u). & ~~
%\end{array}
%\end{align} and $\hat{\theta}$ is efficient, then the estimator,

\item For a random vector $\mathbf{x}$ with PDF $p(\mathbf{x},\theta)$,  $\hat{M}$ and $\hat{u}$ are unique and admissible solution of
\begin{align}\label{eq:28}
\displaystyle{(\hat{M},\hat{u})} &= \displaystyle{\arg~\underset{M,u}{\min}~\underset{\theta\in\mathcal{U}}{\sup}\{MSEB(M,u,\theta)-MSEB(0,0,\theta)\}.}
\end{align}
and for $\hat{M}\neq0$ or $\hat{u}\neq 0$, we have
\begin{align}\label{eq:29}
MSEB(\hat{M},\hat{u},\theta) & < MSEB(0,0,\theta),~\quad \forall~ \theta \in \mathcal{U},
\end{align}
with affine bias estimator $\displaystyle{\hat{\theta}_{b}}$ which achieves the affine MSE bound and have smaller MSE thanc$\hat{\theta}~\forall~\theta\in\mathcal{U}$,
\begin{align}\label{eq:28}
\displaystyle{\hat{\theta}_{b}} &= \displaystyle{(1+M)\hat{\theta}+u}.
\end{align}

\end{itemize}
\end{frame}

\begin{frame}{Mean Square Error Bounds (MSEB) With Affine Bias}
\begin{block}{\textcolor[rgb]{1.0,0.00,0.00}{Proposition 1 [Y. C. Eldar (2008)]}}
Let a random vector $\mathbf{x}$ with pdf $p(\mathbf{x},\theta)$ has the Fisher information of form
\begin{align}
    J^{-1}(\theta) = b^2\theta_0^{2} + 2c\theta_0 +a.
\end{align} Then the minimax M and u that are the solution to \begin{align}
\displaystyle{(\hat{M},\hat{u})} &= \displaystyle{\arg~\underset{M,u}{\min}~\underset{\alpha\in\mathcal{U}}{\sup}\{MSEB(M,u,\theta)-MSEB(0,0,\theta)\}}
\end{align} with $\mathcal{U}= \mathbb{R}$ are given by \begin{align}
M =\left\{\begin{array}{cc}
  -\frac{2b^2}{1+b^2},  & |b|<1 \\
  -1,     & |b|\geq 1,
\end{array}\right.
u =\left\{\begin{array}{cc}
  -\frac{2c}{1+b^2},  & |b|<1 \\
  -\frac{c}{b^2},     & |b|\geq 1.
\end{array}\right.
\end{align} Furthermore, if there exists an efficient estimator $\hat{\theta}$, then
\begin{align}
\hat{\theta}_b =\left\{\begin{array}{cc}
  \frac{1-b^2}{1+b^2}-\frac{2c}{1+b^2},  & |b|<1 \\
  -\frac{c}{b^2},     & |b|\geq 1,
\end{array}\right.
\end{align} has smaller MSE than  $\hat{\theta}~\forall~{\theta}$.
\end{block}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Mean Square Error Of SNR Affine Biased Estimator}
\begin{frame}{Mean Square Error Of SNR Affine Biased Estimator}
\begin{itemize}
    \item The $\hat{\alpha}_{ML}$ does not achieve the CRLB.
    \item To achieve lower bound than CRLB, one need to find the scalar $\hat{M}$ and $\hat{u}$ that are the solution to
    \begin{align}
    \displaystyle{(\hat{M},\hat{u})} &=\displaystyle{\arg~\underset{M,u}{\min}~\underset{\alpha\geq0}{\sup}\{(M\alpha+u)^2+((1+M)^{2}-1)J^{-1}(\alpha)\}}
    \end{align}
    \item Affine biased estimator of the SNR $\alpha$ is
    \begin{align}\label{eq:35}
    \displaystyle{\hat{\alpha}_{b}} &= \displaystyle{(1+M){\hat{\alpha}_{ML}}+u},
    \end{align}
    \item The optimal M and u, that lead to lower MSE than CRLB, can be found using the Semi-definite programming formulation.
    \end{itemize}
\end{frame}


\begin{frame}{Mean Square Error Of SNR Affine Biased Estimator}
\begin{itemize}
    \item Since, for finite N, we have
    \begin{align}
    var\{\hat{\alpha}_{ML}\} = b^2\alpha^2+2c\alpha+a \geq J^{-1}(\alpha)
    \end{align}where
    \begin{align}\nonumber
    a&=\frac{\frac{2}{N}(1-\frac{2}{N})}{N(1-\frac{3}{N})^2(1-\frac{5}{N})}\\
    b&=\sqrt{\frac{2}{N(1-\frac{3}{N})^2(1-\frac{5}{N})}}\\
    c&=\frac{2(1-\frac{2}{N})}{N(1-\frac{3}{N})^2(1-\frac{5}{N})}
    \end{align}
    with $a>0$ for $N>5$.
    \end{itemize}
\end{frame}

\begin{frame}{Mean Square Error Of SNR Affine Biased Estimator}
 \begin{figure}
   \psfrag{N}[c][c][0.7][0]{N}
   \psfrag{a}[c][c][0.7][-90]{a}
   \centering
  \subfloat[The parameter a as the function of N.]{\label{fig:fig2}\includegraphics[width=0.5\textwidth, height=0.5\textheight]{fig2}}~
  \psfrag{b}[c][c][0.7][-90]{b}
  \psfrag{a}[c][c][0.7][0]{N}
  \subfloat[The parameter b as the function of N.]{\label{fig:fig3}\includegraphics[width=0.5\textwidth, height=0.5\textheight]{b_vs_N_plot}}~
\end{figure}
\end{frame}



\begin{frame}{Mean Square Error Of SNR Affine Biased Estimator}
\begin{itemize}
    \item As $\hat{\alpha}_{ML}$ is an efficient estimator of ${\alpha}$, the affine biased estimator of $\alpha$ is, given by \textcolor[rgb]{0.98,0.00,0.00}{Proposition 1},
\begin{align}\label{eq:37}
\hat{\alpha}_{b}&=(1+M)\hat{\alpha}_{ML}+u\\ \nonumber
& =\left\{\begin{array}{cc}
  \frac{(N(1-\frac{3}{N})^2(1-\frac{5}{N})-2)\hat{\alpha}_{ML}-4(1-\frac{2}{N})}{N(1-\frac{3}{N})^2(1-\frac{5}{N})+2},  & |b|<1  (i.e., N$>$10)\\
  -(1-\frac{2}{N}),     & |b|\geq 1,
\end{array}\right.
\end{align}and
\begin{align}\label{eq:38}
MSEB(M,u,\hat{\alpha}_{b}) &< MSE(0,0,\hat{\alpha}_{ML})
\end{align} for $a>0$.\vspace{0.2cm}
 \item[$\sharp$] Here, readily shown that the affine biased estimator outperforms the ML estimator in terms of MSE for finite $N$.
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Conclusion
\section{}
\begin{frame}{Conclusion}
\begin{itemize}
\item ML estimator is asymptotically efficient for large N.\vspace{0.2cm}
\item Affine biased estimator analytically outperform the ML estimator in terms of MSE for finite N.\vspace{0.2cm}
\item The suggested affine biased estimator is realizable for N > 5.\vspace{0.2cm}
\item One can obtain an MSE that is lower than the (unbiased) CRLB over some range of $\alpha$ and N, as described by \textbf{\textcolor[rgb]{0.00,0.00,1.00}{Y. C. Eldar}}.
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% references
\section{}
\begin{frame}{References}
\begin{thebibliography}{99}
\bibitem{1} S. M. Kay,
  \textsl{Fundamentals of Statistical Signal Processing: Estimation Theory},
   Prentice Hall, Upper Saddle River, New Jersey 07458, 1993.
\bibitem{2}Y. C. Eldar,
 \textquotedblleft Uniformly Improving the Cram$\acute{e}$r-Rao Bound and Maximum-Likelihood Estimation,\textquotedblright\ \textsl{IEEE Trans. Signal Process.}, vol. 54, pp. 2943--2956, Aug. 2006.
\bibitem{3}Y. C. Eldar,
 \textquotedblleft MSE Bounds With Affine Bias Dominating the Cram$\acute{e}$r-Rao Bound,\textquotedblright\ \textsl{IEEE Trans. Signal Process.}, Vol. 56, pp. 3824--3836, Aug. 2008.
\end{thebibliography}
\end{frame}

\end{document}

